{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T07:01:37.573656Z",
     "start_time": "2025-03-12T07:01:37.561470Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:03:18.999144Z",
     "start_time": "2025-03-12T07:03:18.971506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('new_train.tsv', header=0, sep='\\t')\n",
    "train_sentences = list(np.array(df_train)[:, 0])\n",
    "train_labels = list(np.array(df_train)[:, 1])\n",
    "df_test = pd.read_csv('new_test.tsv', header=0, sep='\\t')\n",
    "test_sentences = list(np.array(df_test)[:, 0])\n",
    "test_labels = list(np.array(df_test)[:, 1])"
   ],
   "id": "e79cf61728448cbb",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:23:19.910018Z",
     "start_time": "2025-03-12T07:23:19.867235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "1.Basic Embedding Model\n",
    "    1-1. NNLM(Neural Network Language Model)\n",
    "\"\"\"\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "word_list = \" \".join(train_sentences+test_sentences).split()  # 制作词汇表\n",
    "# print(word_list)\n",
    "word_list = list(set(word_list))  # 去除词汇表中的重复元素\n",
    "# print(\"去重后的word_list:\", word_list)\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}  # 将每个单词对应于相应的索引\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}  # 将每个索引对应于相应的单词\n",
    "n_class = len(word_dict)  # 单词的总数\n",
    "\n",
    "# NNLM parameters\n",
    "n_step = 2  # 根据前2个单词预测第3个单词 [0:1]--->[2]\n",
    "n_hidden = 256  # 隐藏层神经元的个数\n",
    "m = 1000  # 词向量的维度"
   ],
   "id": "9a42e640202a1f86",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:23:20.345534Z",
     "start_time": "2025-03-12T07:23:20.316856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "for sentence in train_sentences:\n",
    "    if len(sentence.split()) > max_length:\n",
    "        max_length = len(sentence.split())\n",
    "max_length"
   ],
   "id": "d6bc44f128422ef9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:23:40.779471Z",
     "start_time": "2025-03-12T07:23:40.763529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_batch(sentences, n_step, max_length = 64):\n",
    "    input_batch = torch.zeros(len(sentences), max_length)\n",
    "    target_batch = torch.zeros(len(sentences))\n",
    "    for i in range(len(sentences)):\n",
    "        word = sentences[i].split()\n",
    "        print(word)\n",
    "        input = [word_dict[w] for w in word[:n_step]]\n",
    "        print(input)\n",
    "        target = word_dict[word[n_step+1]]\n",
    "        input_batch[i, 0:len(sentences[i].split())] = torch.tensor(input)\n",
    "        # target_batch[i] = target\n",
    "    return input_batch, target_batch"
   ],
   "id": "b161fd12d4d8e979",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:23:41.691140Z",
     "start_time": "2025-03-12T07:23:41.367085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.C = nn.Embedding(n_class, embedding_dim=m)\n",
    "        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))\n",
    "        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype))\n",
    "        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))\n",
    "        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn(n_class).type(dtype))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C(x)\n",
    "        x = x.view(-1, n_step * m)\n",
    "        # x: [batch_size, n_step*n_class]\n",
    "        tanh = torch.tanh(self.d + torch.mm(x, self.H))\n",
    "        # tanh: [batch_size, n_hidden]\n",
    "        output = self.b + torch.mm(x, self.W) + torch.mm(tanh, self.U)\n",
    "        # output: [batch_size, n_class]\n",
    "        return output\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.C(x)\n",
    "        # print(self.C.weight)\n",
    "\n",
    "\n",
    "\n",
    "model = NNLM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 制作输入\n",
    "input_batch, target_batch = make_batch(train_sentences, n_step)\n",
    "input_batch = Variable(input_batch)\n",
    "target_batch = Variable(target_batch)\n",
    "\n",
    "input_batch"
   ],
   "id": "62aebc13634093d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Even', 'fans', 'of', 'Ismail', 'Merchant', \"'s\", 'work', ',', 'I', 'suspect', ',', 'would', 'have', 'a', 'hard', 'time', 'sitting', 'through', 'this', 'one', '.']\n",
      "[4881, 7757]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (21) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [21].  Tensor sizes: [2]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[111], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# 制作输入\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m input_batch, target_batch \u001B[38;5;241m=\u001B[39m \u001B[43mmake_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_sentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m input_batch \u001B[38;5;241m=\u001B[39m Variable(input_batch)\n\u001B[0;32m     38\u001B[0m target_batch \u001B[38;5;241m=\u001B[39m Variable(target_batch)\n",
      "Cell \u001B[1;32mIn[110], line 10\u001B[0m, in \u001B[0;36mmake_batch\u001B[1;34m(sentences, n_step, max_length)\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m      9\u001B[0m     target \u001B[38;5;241m=\u001B[39m word_dict[word[n_step\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m---> 10\u001B[0m     input_batch[i, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mlen\u001B[39m(sentences[i]\u001B[38;5;241m.\u001B[39msplit())] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# target_batch[i] = target\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m input_batch, target_batch\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The expanded size of the tensor (21) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [21].  Tensor sizes: [2]"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:02:49.580510Z",
     "start_time": "2025-03-12T06:02:48.649483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 开始训练\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(\"Epoch:{}\".format(epoch + 1), \"Loss:{:.3f}\".format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 预测\n",
    "predict = model(input_batch).data.max(\n",
    "    1, keepdim=True)[1]  # [batch_size, n_class]\n",
    "print(\"predict: \\n\", predict)\n",
    "# 测试\n",
    "print([sentence.split()[:3] for sentence in sentences], \"---->\",\n",
    "      [number_dict[n.item()] for n in predict.squeeze()])\n",
    "model.embed(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "for word in word_list:\n",
    "    word_id = word_dict[word]\n",
    "    print(f\"{word} -> {word_id}\")\n",
    "    print(f\"vector of '{word}': {model.embed(torch.tensor([word_id]))}\")"
   ],
   "id": "32a85cafde8e6df3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1000 Loss:0.041\n",
      "predict: \n",
      " tensor([[0],\n",
      "        [2],\n",
      "        [8]])\n",
      "[['i', 'really', 'like'], ['i', 'doubtfully', 'love'], ['i', 'sincerely', 'hate']] ----> ['dog', 'coffee', 'milk']\n",
      "dog -> 0\n",
      "vector of 'dog': tensor([[ 0.7593, -0.3896, -0.6971]], grad_fn=<EmbeddingBackward0>)\n",
      "doubtfully -> 1\n",
      "vector of 'doubtfully': tensor([[-1.1559,  1.8775, -0.9968]], grad_fn=<EmbeddingBackward0>)\n",
      "coffee -> 2\n",
      "vector of 'coffee': tensor([[ 0.6769, -0.5308, -1.5418]], grad_fn=<EmbeddingBackward0>)\n",
      "like -> 3\n",
      "vector of 'like': tensor([[-0.6478,  0.2158,  0.7785]], grad_fn=<EmbeddingBackward0>)\n",
      "sincerely -> 4\n",
      "vector of 'sincerely': tensor([[-0.8695, -1.4554,  1.0802]], grad_fn=<EmbeddingBackward0>)\n",
      "hate -> 5\n",
      "vector of 'hate': tensor([[-0.1706,  0.8658,  1.0102]], grad_fn=<EmbeddingBackward0>)\n",
      "really -> 6\n",
      "vector of 'really': tensor([[-0.7004,  1.5601, -0.5770]], grad_fn=<EmbeddingBackward0>)\n",
      "love -> 7\n",
      "vector of 'love': tensor([[ 0.9230, -0.9520,  0.8691]], grad_fn=<EmbeddingBackward0>)\n",
      "milk -> 8\n",
      "vector of 'milk': tensor([[0.2921, 0.1707, 1.3709]], grad_fn=<EmbeddingBackward0>)\n",
      "i -> 9\n",
      "vector of 'i': tensor([[ 0.9750,  0.1776, -0.7511]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:04:00.345171Z",
     "start_time": "2025-03-12T06:04:00.330627Z"
    }
   },
   "cell_type": "code",
   "source": "sentences[0].split()",
   "id": "bc99f55f45a70623",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'really', 'like', 'dog']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:45:25.730333Z",
     "start_time": "2025-03-12T06:45:25.714869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sen2matrix(sentence):\n",
    "    # to_return = torch.zeros(len(sentence.split()), m, requires_grad=False)\n",
    "    a = torch.tensor([word_dict[word] for word in sentence.split()])\n",
    "    to_return = model.C(a)\n",
    "    return to_return\n",
    "\n",
    "sen2matrix(sentences[0])"
   ],
   "id": "f78a909835960760",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9750,  0.1776, -0.7511],\n",
       "        [-0.7004,  1.5601, -0.5770],\n",
       "        [-0.6478,  0.2158,  0.7785],\n",
       "        [ 0.7593, -0.3896, -0.6971]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:45:28.091338Z",
     "start_time": "2025-03-12T06:45:28.075251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ],
   "id": "b26f1359d9e504ba",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:45:28.743540Z",
     "start_time": "2025-03-12T06:45:28.727709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = torch.tensor([[-1.,1.],[-1.,1.]])\n",
    "corr2d(sen2matrix(sentences[0]), k)\n"
   ],
   "id": "885727b5fa8267d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4631, -3.0658],\n",
       "        [ 3.1241, -1.5744],\n",
       "        [-0.2854,  0.2553]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:45:29.635522Z",
     "start_time": "2025-03-12T06:45:29.624312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ],
   "id": "c8fa0691cb15607a",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T06:51:37.408694Z",
     "start_time": "2025-03-12T06:51:37.392775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1,1, kernel_size=(3, 3), bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），\n",
    "# 其中批量大小和通道数都为1\n",
    "X = sen2matrix(sentences[0]).reshape((1, 1, 4, 3))\n",
    "Y = torch.tensor([[0.],[1.]]).reshape((1, 1, 2, 1))\n",
    "lr = 3e-2  # 学习率\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward(retain_graph=True)\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ],
   "id": "6e0121030063dafa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 0.245\n",
      "epoch 4, loss 0.036\n",
      "epoch 6, loss 0.005\n",
      "epoch 8, loss 0.001\n",
      "epoch 10, loss 0.000\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b46844b5027b4c1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
