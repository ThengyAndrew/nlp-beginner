{
 "cells": [
  {
   "cell_type": "code",
   "id": "f2a12e220a5f10c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:39:26.014067Z",
     "start_time": "2025-03-12T07:39:25.998291Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:08:11.648057Z",
     "start_time": "2025-03-12T08:08:11.584568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('new_train.tsv', header=0, sep='\\t')\n",
    "train_text = list(np.array(df_train)[:, 0])\n",
    "train_labels = list(np.array(df_train)[:, 1])\n",
    "df_test = pd.read_csv('new_test.tsv', header=0, sep='\\t')\n",
    "test_text = list(np.array(df_test)[:, 0])\n",
    "test_labels = list(np.array(df_test)[:, 1])"
   ],
   "id": "c91e094b9c42d7cb",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:27:27.081037Z",
     "start_time": "2025-03-12T08:27:27.013198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 句子数据（简单示例）\n",
    "# sentences = [(\"I love this movie\", 1),\n",
    "#              (\"This is a great day\", 1),\n",
    "#              (\"I hate this weather\", 0),\n",
    "#              (\"This food is terrible\", 0)]\n",
    "\n",
    "train_sentences = list(zip(train_text, train_labels))\n",
    "test_sentences = list(zip(test_text, test_labels))\n",
    "sentences = train_sentences+test_sentences\n",
    "\n",
    "# 构建词典\n",
    "word_list = list(set(\" \".join([s[0] for s in sentences]).split()))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "word_dict[\"<PAD>\"] = 0  # 添加填充标记\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "max_len = max(len(s[0].split()) for s in sentences)\n",
    "\n",
    "# 句子转换为索引（并填充）\n",
    "def sentence_to_tensor(sentence):\n",
    "    idxs = [word_dict[word] for word in sentence.split()]\n",
    "    # 填充到 max_len\n",
    "    idxs += [0] * (max_len - len(idxs))\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ],
   "id": "b3ac9515f096a307",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:45:40.155185Z",
     "start_time": "2025-03-12T08:45:40.142433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 超参数\n",
    "embed_dim = 512  # 词向量维度\n",
    "num_classes = max([i for i in train_labels])+1  # 正向 or 负向\n",
    "\n",
    "# CNN 模型\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(SentimentCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(0.5)  # **新增 Dropout**\n",
    "\n",
    "        # **动态计算 feature_dim**\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randint(0, vocab_size, (1, max_len))  # 随机输入\n",
    "            sample_output = self.pool(torch.relu(self.conv(self.embedding(sample_input).permute(0, 2, 1))))\n",
    "            feature_dim = sample_output.shape[1] * sample_output.shape[2]\n",
    "\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)  # **修正 Linear 层**\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # print(f\"Shape after pooling: {x.shape}\")  # **检查展平前的形状**\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # **展平成 (batch_size, feature_dim)**\n",
    "        x = self.dropout(x)  # **在全连接层前使用 Dropout**\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ],
   "id": "a2d1790c3aa0833",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:45:41.365468Z",
     "start_time": "2025-03-12T08:45:41.146810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建模型\n",
    "model = SentimentCNN(vocab_size, embed_dim, num_classes)\n",
    "\n",
    "# 损失函数 & 优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-4)\n",
    "\n",
    "# 准备数据\n",
    "X_train = torch.stack([sentence_to_tensor(s[0]) for s in train_sentences])\n",
    "y_train = torch.tensor([s[1] for s in train_sentences], dtype=torch.long)\n",
    "X_test = torch.stack([sentence_to_tensor(s[0]) for s in test_sentences])\n",
    "y_test = torch.tensor([s[1] for s in test_sentences], dtype=torch.long)"
   ],
   "id": "b52d7b8a1b9c716",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:54:30.817730Z",
     "start_time": "2025-03-12T08:45:42.123022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练模型\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    test_output = model(X_test)\n",
    "    test_loss = criterion(test_output, y_test)\n",
    "\n",
    "    train_losses.append(loss.item())  # 记录训练损失\n",
    "    test_losses.append(test_loss.item())  # 记录测试损失\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "        print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\", color='blue')\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"Test Loss\", color='red')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Testing Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "c3137bb238183bfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.5722\n",
      "Test Loss: 1.5803\n",
      "Epoch 20, Loss: 1.5274\n",
      "Test Loss: 1.5421\n",
      "Epoch 30, Loss: 1.4998\n",
      "Test Loss: 1.5297\n",
      "Epoch 40, Loss: 1.4873\n",
      "Test Loss: 1.5286\n",
      "Epoch 50, Loss: 1.4720\n",
      "Test Loss: 1.5155\n",
      "Epoch 60, Loss: 1.4512\n",
      "Test Loss: 1.5202\n",
      "Epoch 70, Loss: 1.4391\n",
      "Test Loss: 1.5048\n",
      "Epoch 80, Loss: 1.4245\n",
      "Test Loss: 1.5093\n",
      "Epoch 90, Loss: 1.4125\n",
      "Test Loss: 1.5044\n",
      "Epoch 100, Loss: 1.3941\n",
      "Test Loss: 1.5019\n",
      "Epoch 110, Loss: 1.3864\n",
      "Test Loss: 1.5029\n",
      "Epoch 120, Loss: 1.3661\n",
      "Test Loss: 1.4961\n",
      "Epoch 130, Loss: 1.3584\n",
      "Test Loss: 1.5018\n",
      "Epoch 140, Loss: 1.3383\n",
      "Test Loss: 1.4965\n",
      "Epoch 150, Loss: 1.3209\n",
      "Test Loss: 1.4828\n",
      "Epoch 160, Loss: 1.3091\n",
      "Test Loss: 1.4977\n",
      "Epoch 170, Loss: 1.2892\n",
      "Test Loss: 1.4918\n",
      "Epoch 180, Loss: 1.2774\n",
      "Test Loss: 1.5010\n",
      "Epoch 190, Loss: 1.2618\n",
      "Test Loss: 1.4968\n",
      "Epoch 200, Loss: 1.2529\n",
      "Test Loss: 1.4979\n",
      "Epoch 210, Loss: 1.2319\n",
      "Test Loss: 1.5007\n",
      "Epoch 220, Loss: 1.2171\n",
      "Test Loss: 1.5027\n",
      "Epoch 230, Loss: 1.2018\n",
      "Test Loss: 1.4939\n",
      "Epoch 240, Loss: 1.1861\n",
      "Test Loss: 1.4964\n",
      "Epoch 250, Loss: 1.1780\n",
      "Test Loss: 1.5035\n",
      "Epoch 260, Loss: 1.1595\n",
      "Test Loss: 1.4997\n",
      "Epoch 270, Loss: 1.1492\n",
      "Test Loss: 1.5256\n",
      "Epoch 280, Loss: 1.1356\n",
      "Test Loss: 1.5128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[76], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m output \u001B[38;5;241m=\u001B[39m model(X_train)\n\u001B[0;32m      7\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, y_train)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     11\u001B[0m test_output \u001B[38;5;241m=\u001B[39m model(X_test)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    524\u001B[0m     )\n\u001B[1;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dl_venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:31:05.352354Z",
     "start_time": "2025-03-12T08:31:05.264432Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Test Loss: {criterion(model(X_test), y_test).item():.4f}\")",
   "id": "7d98b37bf1413a4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6394\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T07:52:13.006265Z",
     "start_time": "2025-03-12T07:52:12.981644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 预测\n",
    "test_sentence = test_sentences[0]\n",
    "X_test = sentence_to_tensor(test_sentence).unsqueeze(0)\n",
    "pred = model(X_test).argmax(dim=1).item()\n",
    "print(f\"Sentence: '{test_sentence}', Prediction: {pred}\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Once you get into its rhythm ... the movie becomes a heady experience .', Prediction: 1\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "111be13840acfcae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
